{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13d9706",
   "metadata": {},
   "source": [
    "## **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b360d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Masking, Input, Dense, LSTM, Embedding\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37084ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Go.</th>\n",
       "      <th>Ve.</th>\n",
       "      <th>CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #4986655 (cueyayotl)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corre!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Go.  ... CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986655 (cueyayotl)\n",
       "0   Go.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...                              \n",
       "1   Go.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...                              \n",
       "2   Go.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...                              \n",
       "3   Hi.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #5...                              \n",
       "4  Run!  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...                              \n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table('spa.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1368e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'Go.':'eng', 'Ve.':'spa'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae822dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['eng'].apply(lambda x:x.lower())\n",
    "y = data['spa'].apply(lambda x:x.lower())\n",
    "X = X.apply(lambda x:re.sub(\"[^a-zA-Z]\",\" \",x))\n",
    "y = y.apply(lambda x:re.sub(\"[^a-zA-Z]\",\" \",x))\n",
    "y = y.apply(lambda x:'START_ '+x+' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7f1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab, spa_vocab = set(), set()\n",
    "for sent in X:\n",
    "  for word in sent.split():\n",
    "    if word not in eng_vocab:\n",
    "      eng_vocab.add(word)\n",
    "for sent in y:\n",
    " for word in sent.split():\n",
    "  if word not in spa_vocab:\n",
    "    spa_vocab.add(word)\n",
    "engVocab = sorted(list(eng_vocab))\n",
    "spaVocab = sorted(list(spa_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_length_list=[]\n",
    "for l in X:\n",
    "    source_length_list.append(len(l.split(' ')))\n",
    "max_eng_sent_length= max(source_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5809bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length_list=[]\n",
    "for l in y:\n",
    "    target_length_list.append(len(l.split(' ')))\n",
    "max_spa_sent_length= max(target_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe33b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word2idx = dict([(word, i+1) for i, word in enumerate(engVocab)])\n",
    "spa_word2idx = dict([(word, i+1) for i, word in enumerate(spaVocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b56043d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_idx2word= dict([(i, word) for word, i in  eng_word2idx.items()])\n",
    "spa_idx2word =dict([(i, word) for word, i in spa_word2idx.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf37504",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y, random_state=2)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7283362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(engVocab)\n",
    "num_decoder_tokens = len(spaVocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "161cc8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X=x_train, y=y_train, batch_size=128):\n",
    "  while True:\n",
    "    for i in range(0, len(X), batch_size):\n",
    "      encoder_input_data = np.zeros(shape=(batch_size, max_eng_sent_length), dtype=\"float32\")\n",
    "      decoder_input_data = np.zeros(shape=(batch_size, max_spa_sent_length), dtype=\"float32\")\n",
    "      decoder_output_data = np.zeros(shape=(batch_size, max_spa_sent_length, num_decoder_tokens), dtype=\"float32\")\n",
    "      for j, (input_text, target_text) in enumerate(zip(X[i:i+batch_size], y[i:i+batch_size])):\n",
    "        for k, word in enumerate(input_text.split()):\n",
    "          encoder_input_data[j, k] = eng_word2idx[word]\n",
    "        for k, word in enumerate(target_text.split()):\n",
    "          if k < len(target_text.split())-1:\n",
    "            decoder_input_data[j, k] = spa_word2idx[word]\n",
    "          if k > 0:\n",
    "            decoder_output_data[j, k-1, spa_word2idx[word]] = 1\n",
    "      yield([encoder_input_data, decoder_input_data], decoder_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed5abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(x_train)\n",
    "val_samples = len(x_test)\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "latent_dim=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "458ddf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:54:08.829958: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb_layer = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "enc_lstm_layer = LSTM(units=latent_dim, return_state=True)\n",
    "encoder_outputs, h_state, c_state = enc_lstm_layer(enc_emb_layer)\n",
    "encoder_states = [h_state, c_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d860f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, ))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "dec_lstm_layer = LSTM(units=latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = dec_lstm_layer(dec_emb, initial_state=encoder_states)\n",
    "dec_dense = Dense(units=num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = dec_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e064dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b36c578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('my_models/nmt_eng2spa.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2e88927",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=3,verbose=1,min_delta=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "309685c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [checkpoint, reduceLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8964486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(generate_batch(X=x_train, y=y_train), steps_per_epoch=train_samples//batch_size, epochs=epochs, callbacks=callbacks, verbose=1, validation_data=generate_batch(X=x_test, y=y_test), validation_steps=val_samples//batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
